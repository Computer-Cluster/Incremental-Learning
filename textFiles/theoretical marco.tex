\section{Marco Teórico}

    \subsection{Redes Neuronales Artificiales}
        Las redes neuronales artificiales (RNA) son procesos los cuales contienen simples
        unidades de procesamientos. \\
        Como se puede observar, al mencionar RNA lo primero que se viene a la mente es el 
        procesamiento biol\'ogico por el que transita el cerebro humano. El m\'etodo principal 
        de las redes neuronales es sacarle el máximo poder a los algoritmos de aprendizaje 
        maquina, ya que las redes neuronales tienes un antecedente biol\'ogico.

        Una red de una sola neurona solamente resuelve problemas lienales y tiene la 
        siguiente forma:
        \begin{figure}[H]
            \centering
            \includegraphics[width=\columnwidth]{ANN.jpg}
            \caption{Red Neuronal Artificial B\'asica}
            \label{fig:fig1}
        \end{figure}

        Donde
        $\begin{cases}  
            \Sigma
        \end{cases}$
        es la representaci\'on matem\'atica de la neurona. \\
        Los siguientes valores
        $\begin{cases}
            x_1, x_2, ... , x_n
        \end{cases}$
        señalando a 
        $\begin{cases} \label{eqn: weight}
            w_1, w_2, ... , w_n 
        \end{cases}$ 
        son los datos de entrada que se le dan a la red.
        Cuando la información entra en la neurona aqu\'i se procesan los datos, la operci\'on realizada 
        es una sumatoria ponderada de ellos, dicha ponderaci\'on es asignada a ella  como vemos en la siguiente 
        ecuaci\'on \eqref{eqn: weight}
        Entonces la formula de la sumatoria quedar\'ia de la siguiente manera:
        $\begin{cases} \label{eqn: summation}
            w_1x_1 + w_2x_2 + w_3x_3
        \end{cases}$ \\
        Al verificar bien esta formula, se puede observar que se parece a la operaci\'on de una regresi\'on 
        la cual es:
        $\begin{cases}
            y = w_0 + w_x.
        \end{cases}$
        Internamente la neurona realiza una regresi\'on lineal, el parametro que permite a la neurona 
        moverse verticalmente en la recta se conoce como sesgo, este valor se agrega a la conexi\'on, el cual \
        usualmente se le da un valor de 1. \\
        Agregando este nuevo valor a la formula, queda de la siguiente manera: \\
        $\begin{cases}
            w_1 + w_2 + w_3 + b
        \end{cases}$ 
        donde \textit{b} es el sesgo. \\
        
        Un inconveniente del uso de una sola neurona para experimentos es que solo 
        va a resolver ejercicios parecidos a la puerta l\'ogica AND u OR \label{sec: one neuron}.
        
        \begin{figure}[H]
            \begin{subfigure}[H]{0.49\textwidth}
                \includegraphics[width=\textwidth, height=\textwidth]{and.png}
                \caption{Puerta L\'ogica And}
                \label{fig:f1}
            \end{subfigure}
            \hfill
            \begin{subfigure}[H]{0.49\textwidth}
                \includegraphics[width=\textwidth, height=\textwidth]{or.png}
                \caption{Puerta L\'ogica Or}
                \label{fig:f2}
            \end{subfigure}
            \caption{Puertas L\'ogicas \cite{mcmahon2014}}
        \end{figure}
        
        Pero problemas de tipo XOR no puede, ya que como se nota, una sola neurona sirve para 
        clasificar de un solo lado, asi que no puede clasificar ejercicios
        como los de la imagen \ref{fig:fig3}

        \begin{figure}[H]
            \centering
            \includegraphics[width=5cm]{xor.png}
            \caption{Puerta L\'ogica Xor \cite{mcmahon2014}}
            \label{fig:fig3}
        \end{figure}

        Para solucionarlo se usan dos o m\'as neuronas, adem\'as de la funci\'on de activaci\'on.

            \subsubsection{Funci\'on de Activaci\'on} \label{sec: activation}
                Dicho m\'etodo se utiliza cuando el modelo de RNA contiene dos o m\'as neuronas.
                Esta funci\'on lo que provoca es dar al modelo una salida no lineal, para 
                eso la formula \eqref{eqn: summation} es distorciana para quedar de la siguiente 
                manera: 
                $\begin{cases}
                    f( w_1x_1 + w_2x_2 + w_3x_3 )
                \end{cases}$
                En otras palabras esto es la suma de varias regresiones lineales, lo cual provoca que se obtenga 
                un resultado no linel. \\
                Al hablar de funciones de activaci\'on se deben de comentar las m\'as comunes, como lo es la 
                funci\'on escalonada.
                
                \begin{figure}[H]
                    \centering
                    \includegraphics[width=5cm]{staggered.png}
                    \caption{Funci\'on Escalonada}
                    \label{fig:fig4}
                \end{figure}

                Dicha funci\'on es representada con: 
                
                \[f(x) = \left\{ \begin{array}{lr} 0 & : x < 0\\ 1 & : x \ge 0 \end{array} \right. \]

                La funci\'on sigmoidal, es una de las m\'as comunes, su forma es: 

                \begin{figure}[H]
                    \centering
                    \includegraphics[width=5cm]{sigmoid.png}
                    \caption{Funci\'on Escalonada}
                    \label{fig:fig5}
                \end{figure}

                La cual es representada por la siguiente formula:

                \[f(x) = \sigma(x) =  \left\{ \frac{1}{1 + e^{-x}} \right. \]

                Aparte de agregar deformaciones en la linea. nos ayuda en cuestiones probabilisticas \cite{Freire2021}.\\


        Las redes neuronales presentan demasiadas utilidades las cuales ayudan a  resolver problemas como los 
        siguientes \cite{liu2015}: no linealidad, mapeo entrada-salida, aprendizaje robusto 
        a errores en los datos de entrenamiento, entre otros.

        Existen varios tipos de Redes Neuronales tales como: Redes Neuronales de Perceptr\'on 
        Multicapa, Redes Neuronales Convolucionales, entre otras.
    
        \subsection{Redes Neuronales de Perceptr\'on Multicapa}
            Este tipo de redes entran en el paradigma de Aprendizaje Profundo, ya que como se explic\'o en 
            la secci\'on anterior \eqref{sec:delimitation} aqu\'i la emulaci\'on de las neuronas humanas son m\'as 
            exactas.
            Adem\'as como se explica en esta sección \eqref{sec: one neuron} no es muy recomendado trabajar con una sola 
            neurona por los problemas de tipo XOR, que por lo general es el que se presenta frecuentemente en las problematicas.

            Dichas neuronas se divide en tres capas, capa inicial, oculta y final, en las capas ocultas podemos
            tener m\'as de una fila de neuronas, estas son las encargadas de realizar las operaciones para eliminar 
            la linealidad de los proyectos. 

            En la secci\'on vista anteriormte \eqref{sec: activation} la liealidad se elimina con las funciones de activaci\'on, dicha funci\'on 
            es manipulable si se modifican los parametros de la red, esto permitira que se elabore un plano tridimencional, con este plano tridimencional
            se puede encontrar la soluci\'on al experimento planteado.
            
            Como se puede observar en la siguiente imagen \eqref{fig:fig6} esta red est\'a
            construida por cuatro capas, la cual de ellas dos son ocultas, dentro de esas 
            capas es donde se realiza la ejecuci\'on de las funciones de activaci\'on.

            Cada neurona de color azul va a generar una funci\'on (puede ser sigmoidal, escalonada, entre otras), 
            y cuando se llegue a la capa de salida, cada funci\'on se va a sumar, de esta manera 
            se obtendra una funci\'on no lineal que de resoluci\'on al proyecto.

            \begin{figure}[H]
                \centering
                \includegraphics[width=\columnwidth]{multipercep.jpg}
                \caption{Perceptron Multicapa}
                \label{fig:fig6}
            \end{figure}

            
        \subsection{Algoritmo Backpropagation}
            El bacpkpropagation es el algoritmo donde se explica ¿C\'omo aprende la red?
            Dicho algoritmo fue presentado en 1986, llego a resolver el problema del perceptron 
            ya que ten\'ia muchas limitaciones, lo que provoco que varios experimentos de inteligencia
            artificial se pararan.\\

            Este algortimo permite que la red aprenda por si sola, en 1986 se presento \cite{rumelhart1986} 
            donde muestran un nuevo algoritmo el cual permite que una red neuronal pueda auto-ajustar todos sus 
            parametros para aprender una representaci\'on interna de la informaci\'on que se esta procesando.

            Usando este algoritmo se podra obtener las derivadas parciales del gradiente y del peso, las cuales
            sirven para la optimizaci\'on de la red neuronal, dichas derivadas son:
            \begin{equation*}
                \frac{\partial C}{\partial W^L}
            \end{equation*}
            Pero no se debe de olvidar que tambien se debe de calcular las derivadas del sesgo, la derivada anterior
            queda de la siguiente forma:
            \begin{equation*}
                \frac{\partial C}{\partial b^L}
            \end{equation*}
            donde \textit{L} pertenece al n\'umero de capa donde se encuentra. \\
            Lo que permite a este algoritmo encontrar el error de la derivada es la \textit{chain rule}, la cual en resumen 
            nos da la siguiente ecuaci\'on :
            \begin{equation*}
                C(a(Z^L))
            \end{equation*}

            Donde 
            $\begin{cases}
                Z^L
            \end{cases}$
            representa el resultado de la suma ponderada, y la forma explicita de Z es:
            \begin{equation*}
                Z^L = W^LX + b^L
            \end{equation*}
            , \textit{a} es la funci\'on de activaci\'on 
            y \textit{C} es la funci\'on de Coste.

            Al aplicar la \textit{Chain Rule} se obtiene que las derivadas parciales a obtener son:
            \begin{equation*}
                \frac{\partial C}{\partial w^L} = \frac{\partial C}{\partial a^L} \cdot \frac{\partial a^L}{\partial z^L} \cdot \frac{\partial z^L}{\partial w^L} 
            \end{equation*}
            \begin{equation*}
                \frac{\partial C}{\partial b^L} = \frac{\partial C}{\partial a^L} \cdot \frac{\partial a^L}{\partial z^L} \cdot \frac{\partial z^L}{\partial b^L}
            \end{equation*}

            Como se ve el uso de todas estas derivadas parciales permiten encontrar el error.
            En otras palabras lo que realiza dicho algoritmo es terminar un proceso, si se encuentra un error, esta va a regresar
            hasta la neurona que da error, pero regresa cambiando el valor de \textit{w}, este proceso se va a 
            repetir hasta encontrar el error perfecto, el cual es donde el error disminuye a lo m\'as bajo y el resultado
            de la red es lo m\'as acertado.

            Como se observa esta metodolog\'ia es muy \'util en el uso de redes neuronales, es por eso que 
            se usa en la siguiente investigaci\'on \cite{bullinaria2009} es usada para obtener un buen resultado
            en el aprendizaje incremental, como es explicado ah\'i es usado para que las redes obtengan una buena topolog\'ia con 
            buena actualizaci\'on de pesos.

        \subsection{Redes Neuronales Convolucionales}

            Este tipo de red trabaja con el uso de imágenes, por lo general de alta calidad, el \'unico problema que se tiene 
            al momento de que sean de alta resoluciones son que el tiempo de entrenamiento sea enorme y el tiempo de testo (Tengo duda de que la palabra testo quede, puedes checarlo? si no pues dejalo asi jsjs)sea muy tardío.

            Consta de diversas multicapas alternadas, al final tiene una red perceptr\'on multicapa. (REVISEN LA REDACCIÓN, ESTÁ MUY SEGMENTADA LAS IDEAS, TIENEN QUE EXPLICAR UN POCO MÁS PARA QUE SE ENTIENDA MEJOR, TOMEN COMO BASE LA REDACCIÓN QUE LES PONGO AL INICIO DEL TRABAJO, VEAN CÓMO VOY A EXPLICANDO MÁS COSAS, Y AUNQUE ME LLEVO MÁS ESPACIO, QUEDA MEJOR)
            La entrada de una red convolucional, con diferentes medidas en altura y anchura de imagen, para el uso 
            de los proyectos se trabajan en escalas de grises, las cuales contienen filtros y cada filtro tiene distintos 
            rasgos y características de tamaño. Cada capa es submuestreo de m\'inimo a m\'aximo, muestra donde se toman valores 
            desde 2 im\'agenes pequeñas hasta no mas de 5 im\'agenes grandes.

            Antes o despu\'es del submuestreo se aplica la activaci\'on sigmoidal para cada mapeo de rasgos \cite{duran2017}.

            \begin{figure}[H]
                \centering
                \includegraphics[width=\columnwidth]{esquemaRedConvolucional.png}
                \caption{Esquema de una Red Convolucional \cite{duran2017}}
                \label{fig:fig7}
            \end{figure}

Así como las redes convolucionales, también existen las redes profundas(deep learning), sin embargo, como se comenta en esta sección \ref{sec:delimitation} el presente trabajo solo se basará en redes del tipo multicapa perceptron usando el algoritmo de bacpkpropagation, donde de ser demostrados los principios descritos, se podrá aplicar a cualquier otro tipo de algoritmo de aprendizaje.  (LUIS Y SANDRA, AGREGUEN A LA DELIMITACIÓN LO QUE ESTOY INDICANDO AQUÍ, PARA QUE TENGA SENTIDO ESTO)



\subsection{Aprendizaje}

\subsubsection{Aprendizaje en humanos}
        El humano tiene una forma de aprendizaje muy particular, la cual se basa del estudio, donde lee, escribe y practica acerca de
        su tema de interés, pero dicho aprendizaje se puede ir olvidando, esta es una acción muy común que a cualquier persona le sucede.
        Existen estudios donde se comenta que existen tres motivos del porque se olvidan las cosas, proviene parte de la regularización de las emociones,
        el como se adquirieron los conocimientos, y porque el olvido es un proceso por el cual el ser humano transita a lo largo de su vida \cite{Nrby2015}. Pero cabe
        mencionar que esto no es lo único que causa la perdida de memoria, ya que existe la déficit de memoria. 

    \subsubsection{Aprendizaje Humano}
        Al momento de hablar del aprendizaje humano, se debe de hablar de la ciencia cognitiva, que es quien se encarga de descubrir esta incógnita,
        esta ciencia lo estudia de un modo multidisciplinario, el cual abarca las \'areas de \cite{bransford2000}: antropología, lingüística, 
        filosofía, sicología del desarrollo, ciencia de la computación, neurociencia.
        Con el método de esta ciencia se pueden descubrir dos tipos de aprendizaje que son: el aprendizaje con compresi\'on y el aprendizaje Activo.
        
        \subsubsection{Aprendizaje con Compresi\'on}
            La comprensi\'on es una actividad la cual se ha generado al momento de realizar cualquier tipo de lectura.\\
            Teniendo un enfocamiento en el \'ambito estudiantil, ya que es donde m\'as se maneja esta t\'actica, esto es una
            practica algo compleja, sistemática y organizada, pues da el significado de la literatura, gracias a esto se puede
            obtener el contexto de la literatura.
            (Insisto que lo anterior esta mal dicho, pero no se me ocurre de que manera expresarlo)

            Al conocer esto se puede decir con seguridad que para cualquier tipo de aprendizaje la comprensi\'on es 
            una parte primordial \cite{perez2014}.
            (Tambien siento que se debe de agregar un concepto en si o como dice Landassuri mas explicado)

        \subsubsection{Aprendizaje Activo}
            El aprendizaje de la forma en la que se conoce no es del todo efectiva, ya que el sistema educativo
            no se basa en el principio de \textit{belongingness}, el cual esta asociado al estimulo con su respuesta,
            y esto es lo m\'as importante para que el ser humano pueda aprender cualquier cosa.\\
            Este tipo de aprendizaje se basa en la recepci\'on de conocimientos y la pr\'actica donde se ponen en marcha los conocimientos adquiridos.\\
            Otro concepto importante aqu\'i es la tautolog\'ia doble (\textit{selbstt\"atiges Lernen}), que en palabras informales es convertirse en autodidacta, 
            se puede observar que esto pertenece a dicho aprendizaje, porque usa el principio mencionado anteriormente \cite{Huber2008}.
   

 \subsubsection{Aprendizaje Incremental}
        Con el pasar de los años la tecnología a evolucionado, eso quiere decir que el Aprendizaje Automático se ha actualizado y que la 
        cantidad de datos va aumentado con más frecuencia.
        
        Se puede verificar como \textit{"Una tarea de aprendizaje es incremental si los ejemplos de entrenamiento usados para 
        resolverla están disponibles en horas extras, generalmente uno a la vez"} \cite{GiraudCarrier2000}, si los resultados no se 
        necesitan de manera urgente, este tipo de trabajos serán resueltos por algoritmos de aprendizaje no incremental. 

        Una área donde esto es de mucha utilidad es la \textit{Rob\'otica} porque este necesita estar en constante entrenamiento \cite{GiraudCarrier2000}.

        Dicha forma de aprender fue inspirada en la forma en que el humano aprende y esta más rápida, fue por esto que fue adoptada 
        por el aprendizaje m\'aquina.

        Con el paso del tiempo se ha convertido en un paradigma del aprendizaje automático, aquí el aprendizaje toma el lugar de nuevos ejemplos para juntarlos 
        y conforme van aprendiendo estos toman el lugar de los ejemplos ya aprendidos \cite{liu2015}.

        \subsubsection{Algoritmos de Aprendizaje Incremental}
            \textit{"Un algoritmo de aprendizaje es incremental si,
            para cualquier muestra de entrenamiento dada:
            \begin{equation}
                e_{1} , .... , e_{s}
			\end{equation}
            produce un secuencia de hipótesis 
            \begin{equation}
                h_{0} , h_{1}, . . . , h_{n} 
            \end{equation}
            tal que hi+1 depende solo de hola(Tengo duda con la palabra hola si lo tradujiste fijate bien que hayas copiado bien, luego los simbolos no los copia como es ) y del ejemplo actual e"} \cite{GiraudCarrier2000}, como se 
            observa, estos son algoritmos que permiten a la inteligencia artificial poder realizar actividades de predicci\'on 
            de una manera m\'as eficaz.\\
            Un ejemplo del uso de esta rama es el proyecto \textit{COBWEB}, donde se trata de categorizar el n\'umero de Cl\'uster y la pertenencia 
            de dichas categor\'ias por medio de una m\'etrica probabil\'istica global, esto lo realiza por medio de que se agrega 
            una nueva categor\'ia, este proceso lo que realizar\'a es actualizar todas las probabilisticas con los nuevos datos recabados \cite{fisher1987}.


AQUÍ FALTA DESCRIBIR A DETALLE CÓMO SE LLEVA ACABO EL APRENDIZAJE INCREMENTAL, SE PUEDEN BASAR EN EL TRABAJO DE BULLINARIA Y LEANR ++, EXPLIQUENLO A DETALLE PARA QUE SEA SU ESTADO DEL ARTE

