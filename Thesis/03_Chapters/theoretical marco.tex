\section{Marco Teórico}

\subsection{Redes Neuronales Artificiales}

Las redes neuronales artificiales (RNA) son modelos computacionales dentro de la Inteligencia Artificial que contienen unidades de procesamiento simples llamadas neuronas. Estas se inspiran en el cerebro humano, basándose en la conectividad entre neuronas y el aprendizaje que pueden tener. Un perceptrón o neurona (artificial) solo resuelve problemas lineales y tiene la siguiente forma:

\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{ANN.jpg}
    \caption{Red Neuronal Artificial Básica}
    \label{fig:fig1}
\end{figure}

Donde $\Sigma$ es la representación matemática de la neurona. $x_1$, $x_2$, \dots ,$x_n$ son las variables de entrada a la red, y $w_1$, $w_2$, \dots ,$w_n$ son los pesos con los cuales se ponderan las entradas, es decir, se multiplican cuando la información entra en la neurona. Posteriormente, se suman todos esos valores: $w_1 x_1 + w_2 x_2 + w_3 x_3$. 

Al revisar esta fórmula, se puede observar que se parece a la operación de una regresión, la cual es: $y = w_0 + w_i x_i$. De esta manera, internamente la neurona realiza una regresión lineal. El parámetro que permite a la neurona trazar una recta cruzando el eje $y$ en el plano cartesiano (eje de las ordenadas) es conocido como sesgo (del inglés $bias$). Este valor se agrega a la conexión y usualmente se le asigna un valor de 1.

Agregando este nuevo valor a la fórmula, queda de la siguiente manera: $y = \Sigma w_i x_i + w_0 b$, donde $b$ es el sesgo. 

Un inconveniente de utilizar una sola neurona en experimentos es que solo resuelve problemas lineales, como las puertas lógicas AND o OR:

\begin{figure}[H]
    \begin{subfigure}[H]{0.49\textwidth}
        \includegraphics[width=\textwidth, height=\textwidth]{and.png}
        \caption{Puerta Lógica AND}
        \label{fig:f1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[H]{0.49\textwidth}
        \includegraphics[width=\textwidth, height=\textwidth]{or.png}
        \caption{Puerta Lógica OR}
        \label{fig:f2}
    \end{subfigure}
    \caption{Puertas Lógicas}
\end{figure}

Sin embargo, no puede resolver problemas como el XOR, ya que una sola neurona clasifica solo de un lado, lo que la hace incapaz de clasificar correctamente ejercicios como el de la Figura \ref{fig:fig3}.

\begin{figure}[H]
    \centering
    \includegraphics[width=5cm]{xor.png}
    \caption{Puerta Lógica XOR}
    \label{fig:fig3}
\end{figure}

Para solucionar esto, se utilizan dos o más neuronas, además de una función de activación que permite pasar la información de una neurona a otra dentro de un rango específico, lo cual se describirá en la siguiente sección.

\subsubsection{Función de Activación} \label{sec:activation}

Este método se utiliza cuando el modelo de RNA contiene dos o más neuronas. La función de activación da al modelo una salida no lineal. Para ello, la segunda fórmula presentada se distorsiona de la siguiente manera: $f(w_1x_1 + w_2x_2 + w_3x_3 + b_0)$ para el caso de 3 entradas.

Al hablar de funciones de activación, se deben comentar las más comunes, como la función escalonada.

\begin{figure}[H]
    \centering
    \includegraphics[width=5cm]{staggered.png}
    \caption{Función Escalonada}
    \label{fig:fig4}
\end{figure}

Dicha función está representada por:

\[
f(x) = \left\{ \begin{array}{lr} 
0 & : x < 0 \\
1 & : x \ge 0 
\end{array} \right.
\]

La función sigmoide es una de las más comunes, y su forma es:

\begin{figure}[H]
    \centering
    \includegraphics[width=5cm]{sigmoid.png}
    \caption{Función Sigmoide}
    \label{fig:fig5}
\end{figure}

Está representada por la siguiente fórmula:

\[
f(x) = \sigma(x) =  \frac{1}{1 + e^{-x}}
\]

Aparte de realizar dicha transformación, las funciones de transferencia ayudan en cuestiones probabilísticas, ya que se representan en un rango de 0 a 1. Además, la función sigmoide es diferenciable, lo que permite al algoritmo de retropropagación llevar a cabo el entrenamiento.

La función Unidad Rectificada Lineal (ReLU) es una función lineal que, cuando es positiva, toma el valor de la entrada, y cuando es negativa, toma el valor de 0. Su forma es:

\begin{figure}[H]
    \centering
    \includegraphics[width=6cm]{relu.png}
    \caption{Función ReLU}
    \label{fig:fig6}
\end{figure}

Está representada por la siguiente fórmula \cite{Freire2021}:

\[
f(x) = \left\{ \begin{array}{lr} 
0 & : x < 0 \\
x & : x \ge 0 
\end{array} \right.
\]

La función Softmax transforma las salidas en una representación probabilística, de tal manera que la suma de todas las probabilidades sea 1. Su gráfico es:

\begin{figure}[H]
    \centering
    \includegraphics[width=6cm]{softmax.png}
    \caption{Función Softmax}
    \label{fig:fig7}
\end{figure}

Su representación matemática es \cite{calvo-2018}:

\[
f(z)_j = \frac{e^{z_j}}{\sum_{K=1}^{K} e^{z_k}}
\]

Las redes neuronales presentan diversas utilidades que ayudan a resolver problemas como no linealidad, mapeo entrada-salida, aprendizaje robusto a errores en los datos de entrenamiento, entre otros. Existen varios tipos de redes neuronales, como las redes neuronales de perceptrón multicapa y redes neuronales convolucionales, que se describirán brevemente más adelante.

\subsection{Redes Neuronales de Perceptrón Multicapa}

Las Redes Neuronales de Perceptrón Multicapa (MLP) pueden dividirse en dos capas (entrada y salida), pero también pueden tener tres o más capas (entrada, una o más capas ocultas y salida). En las capas ocultas, se pueden tener más de una fila de neuronas, que son las encargadas de realizar las operaciones para eliminar la linealidad de los datos. Como se comentó anteriormente en la sección \ref{sec:activation}, la linealidad de los datos se elimina con las funciones de activación, que modifican los parámetros de la red, permitiendo la elaboración de un plano tridimensional con el cual se puede encontrar la solución al problema planteado.

Además, como se explicó anteriormente, no es recomendable trabajar con una sola neurona debido a los problemas al resolver tareas como XOR, donde se requieren dos líneas rectas para clasificar el problema correctamente.

Como se puede observar en la Figura \ref{fig:fig8}, para este caso se cuenta con una MLP que consta de 4 capas: una de entrada, dos ocultas y una de salida. En las capas ocultas y de salida se lleva a cabo el procesamiento de las funciones de activación, mientras que en la capa de entrada no se aplica ninguna función de transferencia, ya que simplemente representa las entradas al modelo.

\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{multipercep.jpg}
    \caption{Perceptrón Multicapa}
    \label{fig:fig8}
\end{figure}

\subsection{Algoritmo de Retropropagación}

El algoritmo de retropropagación es un algoritmo de aprendizaje que permite que una red neuronal autoajuste todos sus parámetros para aprender una representación interna de la información que está procesando. Llegó para resolver la limitante del perceptrón, que solo resuelve problemas lineales, y se extiende a redes más complejas, es decir, a problemas no lineales.

Mediante este algoritmo se obtienen las derivadas parciales del gradiente y los pesos, los cuales se utilizan para optimizar la red neuronal. También se deben calcular las derivadas del sesgo, que indican en qué capa se encuentra el error. El uso de estas derivadas parciales permite encontrar el error, y lo que realiza el algoritmo es retroceder hasta la neurona donde se encuentra el error, regresando desde la capa de salida hasta la capa de entrada.

Para poder realizar todo este proceso de retropropagación, es necesario contar con una función de activación diferenciable.

\subsection{Redes Neuronales Convolucionales}

Las Redes Neuronales Convolucionales (CNN, por sus siglas en inglés) son redes profundas con una estructura especial. Están conformadas por tres tipos de capas: convolucionales, de agrupación y completamente conectadas.

En las capas convolucionales, el filtro detecta características específicas dentro de la imagen de entrada, como bordes, colores, o texturas, y genera un mapa de características. En las capas de agrupación, se reduce la resolución espacial de la imagen, lo que permite reducir la cantidad de parámetros y el costo computacional. Finalmente, las capas completamente conectadas están encargadas de realizar la clasificación.

Las CNN son muy útiles para el reconocimiento de imágenes, y se han utilizado en una amplia variedad de aplicaciones, desde la visión por computadora hasta la medicina y el reconocimiento de voz.


\subsection{Aprendizaje}

\subsubsection{Aprendizaje en humanos}

El ser humano tiene una forma de aprendizaje muy particular, basada en el estudio, donde lee, escribe y practica sobre su tema de interés. Sin embargo, este aprendizaje puede ser olvidado, un fenómeno común que ocurre a cualquier persona. Existen estudios que sugieren tres razones principales por las cuales olvidamos las cosas: la regularización de las emociones, la forma en que adquirimos los conocimientos, y el hecho de que el olvido es un proceso natural que forma parte de la vida humana \cite{Nrby2015}. Cabe mencionar que el olvido no es la única causa de la pérdida de memoria, ya que también puede existir un déficit de memoria.

\subsubsection{Aprendizaje Humano}

Al hablar del aprendizaje humano, es fundamental abordar la ciencia cognitiva, que se encarga de investigar este fenómeno desde un enfoque multidisciplinario. Esta ciencia abarca diversas áreas como \cite{bransford2000}: antropología, lingüística, filosofía, psicología del desarrollo, ciencia de la computación y neurociencia. El estudio de la cognición permite distinguir entre dos tipos de aprendizaje: el aprendizaje con comprensión y el aprendizaje activo.

\subsubsection{Aprendizaje con Comprensión}

La comprensión es una actividad fundamental que ocurre durante cualquier tipo de lectura. En el ámbito educativo, esta práctica es compleja, sistemática y organizada, ya que otorga significado a la literatura y favorece la durabilidad del aprendizaje. Con base en esto, podemos afirmar que la comprensión es un aspecto primordial en cualquier tipo de aprendizaje \cite{perez2014}.

\subsubsection{Aprendizaje Activo}

El aprendizaje tradicional, tal como se conoce en muchos sistemas educativos, no siempre es efectivo, ya que no se basa en el principio de \textit{belongingness}, que está relacionado con la conexión entre el estímulo y su respuesta, lo cual es crucial para que el ser humano aprenda de manera efectiva. Este tipo de aprendizaje implica la recepción de conocimientos y la práctica activa de estos, utilizando los conocimientos adquiridos. Un concepto clave relacionado con este enfoque es la \textit{tautología doble} (\textit{selbstt\"atiges Lernen}), que, en términos informales, se refiere al acto de convertirse en autodidacta. Este tipo de aprendizaje se basa en el principio mencionado anteriormente \cite{Huber2008}.

\subsubsection{Aprendizaje Incremental}

Con el paso del tiempo, la tecnología ha evolucionado, lo que ha llevado a un aumento en la cantidad de datos disponibles. El aprendizaje automático ha experimentado avances significativos, y los datos se generan y procesan con mayor frecuencia.

Se puede definir una tarea de aprendizaje como incremental si los ejemplos de entrenamiento utilizados para resolverla se presentan de manera secuencial, generalmente uno a la vez. Si los resultados no son urgentes, este tipo de tareas puede ser resuelto mediante algoritmos de aprendizaje no incremental \cite{GiraudCarrier2000}. Un área donde el aprendizaje incremental es especialmente útil es en la \textit{robótica}, ya que estos sistemas requieren entrenamiento constante \cite{GiraudCarrier2000}.

Este enfoque de aprendizaje fue inspirado por la forma en que los seres humanos aprenden y es más rápido, lo que llevó a su adopción en el campo del aprendizaje automático.

Con el tiempo, el aprendizaje incremental se ha convertido en un paradigma del aprendizaje automático, donde el sistema toma nuevos ejemplos y los agrega a los ya aprendidos. A medida que el sistema aprende, los ejemplos previos pueden ser reemplazados por los nuevos \cite{liu2015}.

\subsubsection{Algoritmos de Aprendizaje Incremental}

Un algoritmo de aprendizaje incremental se define por los siguientes criterios:
\begin{enumerate}
    \item Ser capaz de aprender y actualizarse con cada nuevo dato, etiquetado o no etiquetado.
    \item Conservar el conocimiento adquirido previamente.
    \item No requerir acceso a los datos originales.
    \item Ser capaz de generar nuevas clases o clusters cuando sea necesario, así como dividir o fusionar clusters según lo requiera el entorno.
    \item Tener una naturaleza dinámica, adaptándose a un entorno cambiante \cite{Deshmukh2013}.
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=\columnwidth]{MetodosAprendizajeIncremental.png}
    \caption{Dos enfoques tradicionales del aprendizaje incremental.}
    \label{fig:fig9}
\end{figure}

Como se observa en la Figura 11, el primer enfoque consiste en la acumulación de datos, donde, al recibir una nueva porción de datos \(D_j\), se descarta la hipótesis \(h_{j-1}\) y se genera una nueva hipótesis \(h_j\) basada en todos los datos disponibles hasta ese momento. En el segundo enfoque, al recibir una nueva porción de datos \(D_j\), se desarrolla una única hipótesis nueva o un conjunto de hipótesis nuevas basadas en los nuevos datos. Finalmente, se puede usar un mecanismo de votación para combinar todas las decisiones de las diferentes hipótesis y obtener la predicción final.

El aprendizaje incremental tiene la ventaja de no requerir almacenamiento de los datos previos, ya que el conocimiento se guarda en las hipótesis generadas durante el proceso de aprendizaje.

\begin{quote}
\textit{"Un algoritmo de aprendizaje es incremental si, para cualquier muestra de entrenamiento dada:
\begin{equation*}
    e_1, e_2, ..., e_s
\end{equation*}
produce una secuencia de hipótesis
\begin{equation*}
    h_0, h_1, ..., h_n
\end{equation*}
tal que \(h_{i+1}\) depende solo de \(h_i\) y de la muestra actual \(e\)" \cite{GiraudCarrier2000}.}
\end{quote}

Como se observa, estos algoritmos permiten que la inteligencia artificial realice tareas de predicción de manera más eficiente.

Un ejemplo de esta metodología es el proyecto \textit{COBWEB}, que categoriza el número de clusters y la pertenencia de dichos clusters utilizando una métrica probabilística global. Este proceso implica agregar nuevas categorías, actualizando las probabilidades con los nuevos datos recolectados \cite{fisher1987}.